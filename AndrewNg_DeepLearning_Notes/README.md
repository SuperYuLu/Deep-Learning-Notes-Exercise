# Deeplearning.ai Course Series Notes  
This is a collection of hand written notes on course series  "Deep Learning Specialization" from deeplearning.ai, by Andrew Ng.  

Anyone is free to use/distribute the notes, the original rights of knowledge belongs to deeplearning.ai and Andrew Ng.  

## Course1-Neural Networks and Deep Learning  
------   

- [WEEK1: Intro to Deep Learning](https://github.com/SuperYuLu/Deep-Learning-Notes-Exercise/blob/master/AndrewNg_DeepLearning_Notes/Course1-Neural%20Networks%20and%20Deep%20Learning/WEEK1-Intro%20to%20Deep%20Learning.pdf)
  + What is neural network 
  + Supervised learning V.S. nerual network
  + Why is deep learning taking-off?
- [WEEK2: Neural Networks Basics](https://github.com/SuperYuLu/Deep-Learning-Notes-Exercise/blob/master/AndrewNg_DeepLearning_Notes/Course1-Neural%20Networks%20and%20Deep%20Learning/WEEK2-Neural%20Networks%20Basics.pdf)
  + Logistic regression
  + Vectorize gradient descent 
  + Broadcasting in Python 
  + Explaination on cost function 
- [WEEK3: Shallow Neural Networks](https://github.com/SuperYuLu/Deep-Learning-Notes-Exercise/blob/master/AndrewNg_DeepLearning_Notes/Course1-Neural%20Networks%20and%20Deep%20Learning/WEEK3-Shallow%20Neural%20Network.pdf)
  + Neural network overview 
  + Activation functions (sigmoid, tanh, ReLU, Leaky ReLU)
  + Why need non-liner activation function? 
  + Forward / Backward propogation
  + Vectorized backward propogation
  + Random initialization 

- [WEEK4: Deep Neural Networks](https://github.com/SuperYuLu/Deep-Learning-Notes-Exercise/blob/master/AndrewNg_DeepLearning_Notes/Course1-Neural%20Networks%20and%20Deep%20Learning/WEEK4-Deep%20Neural%20Network.pdf)
  + Deep L-layer neural network  
  + Why deep ? circuit theory 
  + Building blocks for deep neural networks 
  + Forward and backward propogation 
  + Parameters and hyperparameters 
- Notes info:
  + Start: 02/10/2018
  + Finish: 03/10/2018
  
## Course2-Improving Deep Neural Networks  
------  

- [WEEK1: Practical Aspect of Deep Learning](https://github.com/SuperYuLu/Deep-Learning-Notes-Exercise/blob/master/AndrewNg_DeepLearning_Notes/Course2-Improving%20Deep%20Neural%20Networks/Week1-Practical%20Aspect%20of%20Deep%20Learning.pdf)
  + Setting up ML aplication 
  + Regularizing deep neural network
  + Regularization: L2 norm, L1 norm
  + Droupout regularization
  + Data augmentation 
  + Normalizing Inputs 
  + Vanishing / exploding gradients
  + Weight random initialization 
  + Gradient checking

- [WEEK2: Optimization Algorithms](https://github.com/SuperYuLu/Deep-Learning-Notes-Exercise/blob/master/AndrewNg_DeepLearning_Notes/Course2-Improving%20Deep%20Neural%20Networks/Week2-Optimization%20Algorithms.pdf)
  + Mini-batch gradient descent
  + Expontially weighted averages & bias correction 
  + Gradient descent with momentum 
  + RMSprop
  + Adam Algorithm 
  + Learning rate decay
  + Problem of local optimal, plateaus
- [WEEK3: Hyperparameter Tuning, Batch Normalization and Programming Frameworks]
  + Hyperparameter tuning process, suggestions 
  + Batch normalization and realization in NN
  + Softmax multi-class classification 
  + Deep learning frameworks, TensorFlow
- Notes info:
  + Start: 03/24/2018
  + Finish: 06/24/2018

## Course3-Structuring Machine Learning Projects
------  
+ In progress 
